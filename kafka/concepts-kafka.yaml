- Kafka Topics:
  - Topics: a particular stream of data 
  - Like a table in a database (without all the constraints)
  - You can have as many topics as you want 
  - A topic is identified by its name
  - Any kind of message format
  - The Sequence of messages is called a data stream 
  - You cannot query topics, instead, use kafka producers to send data and kafka consumers to read the data
  - Data is kept only for a limited time (default is one week - configurable)
  - E.g., logs, truck gps etc 

  - Partitions and offsets:
    - Topics are split in partitions (example- 100 partitons)
    - Messages with each partition are ordered
    - Each message within a partition gets and incremental id, called offset 

    - Kafka topics are immutable. once data is written to a partition, it cannot be changed 
    
    - Offset only have a meaning for a specific partition:
      - E.g., offset 3 in partition 0 is not same data as offset 3 in partition 1 
      - Offsets are not re-used even if previous messages have been deleted 
    
    - Order is guranteed only within a partition (not across partitions)
    - Data is assigned randomly to a partition unless a key is provided (more on this later)
    - You can have as many partitions per topic as you want
  
  - Topic replication factor:
    - Topics should have a replication factor > 1 (usually between 2 and 3)
    - This way if a broker is down, another broker can serve the data 

    - Concept of Leader for a partition: 
      - At any time only ONE broker can be a leader for a given partition 
      - Producers can only send data to the broker that is leader of a partition 
      - The other brokers will replicate the data 
      - Therefore, each partition has one leader and multiple ISR (in-sync replica)
      - Kafka Producers can only write to the leader broker for a partition 
      - Kafka consumers by default will read from the leader broker for a partition 

      - Kafka Consumers Replica Fetching (Kafka v2.4+):
        - Since Kafka 2.4, it is possible to configure consumers to read from the closest replica 
        - This may help improve latency, and also decrease network costs if using the cloud 
      
  - Kafka Topic Durability:
    - For a topic replication factor of 3, topic data durability can withstand 2 brokers loss 
    - As a rule, for a replication factor of N, you can permanently lose up to N-1 brokers and still recover your data 


- Producers and Keys:
  - Producers write data to topics (which are made of partitions)
  - Producers know to which partition to write to (and which kafka broker has it)
  - In case of kafka broker failures, Producers will automatically recover
  - Producer load is load balanced to many brokers thanks to the number of partitions 

  - Message Keys: 
    - Producers can choose to send a key with a message (string, number, binary, etc..)
    - If key==null, data is sent round robin (partition 0, then 1, then 2...) - load balancing 
    - If key!=null, then all messages for that key will always go to the same partition (hashing)
    - A key are typically sent if you need message ordering for a specific field (ex- truck_id)
  
  - Kafka Message Anatomy:
    - Key-binary / value-binary (can be null)
    - Compression Type (none, gzip, snappy, lz4, zstd)
    - Headers (Optional) [key, value]
    - Partition + Offset 
    - Timestamp (set by user or system)
  
  - Kafka Message Serializer:
    - Kafka only accepts bytes as an input from producers and sends bytes out as an output to consumers 
    - Do Message Serialization -> means transforming objects / data into bytes 
    - They are used on the values and the key 
    - Common Serializers
      - String (incl json)
      - Int, Float 
      - Avro
      - Protobuf
  
  - Producer Acknowledgements (acks):
    - Producers can choose to receive acknowledgements of data writes:
      - acks=0: Producer won't wait for acknowledgement (possible data loss)
      - acks=1: Producer will wait for leader acknowledgement (limited data loss)
      - acks=all: Leader + replicas acknowledgement (no data loss)

    
- Consumers:
  - Consumers read data from a topic (identified by name) - pull model
  - Consumers automatically know which broker to read from 
  - In case of broker failures, consumers know how to recover 
  - Data is read in order from low to high offset within each partitions 

  - Consumer Deserializer:
    - Deserialize indicates how to transform bytes into objects / data 
    - They are used on the value and the key of the message 
    - Common Deserializers:
      - String(incl json)
      - int, float, Avro, Protobuf 

  - The serialization / deserialization type must not change during a topic lifecycle (create a new topic instead)

  - Consumer Groups:
    - All the consumers in an application read data as a consumer groups 
    - Each consumer within a group reads from exclusive partitions 
    - If you have more consumers than partitions, some consumers will be inactive 
    - In Apache Kafka it is acceptable to have multiple consumer groups on the same topic 
    - To create distinct consumer groups, use the consumer property group.id 
    
  - Consumer offsets:
    - Kafka stores the offsets at which a consumer group has been reading 
    - The offsets committed are in kafka topic __consumer_offsets 
    - When a consumer in a group has processed data received from kafka, it should be periodically comming the offsets 
      (the kafka broker will write to __consumer_offsets, not the group itself)
    - If a consumer dies, it will be able to read back from where it left off thanks to the committed consumer offsets
  
  - Delivery semantics for Consumers: 
    - At least once (default):
      - Offsets are committed after the message is processed 
      - This can result in duplicate processing of messages, Make sure your processing is Idempotent 
    - At most once:
      - Offsets are committed as soon as messages are received 
      - If the processing goes wrong, some messages will be lost (they won't be read again)
    - Exactly once: 
      - For kafka->kafka workflows, use the transaction api (easy with kafka streams api)
      - For kafka->External system workflows, use an Idempotent consumer 
  

- Kafka Brokers:
  - A kafka cluster is compose of multiple brokers (servers, they send and receive data)
  - Each broker is identified with its ID (integer)
  - Each broker contains certain topic partitions (distributed)
  - After connecting to any broker (called a bootstrap broker), you will be connected to the entire cluster 
    (kafka clients have smart mechanics for that)
  - A good number to get started is 3 brokers, but some big clusters have over 100 brokers 


- Zookeeper:
  - Zookeeper manages brokers (keeps a list of them)
  - Zookeeper helps in performing leader election for partitions 
  - Zookeeper sends notifications to kafka in case of changes 
    (e.g., new topic, broker dies, broker comes up, delete topics, etc...)
  - Kafka 2.x can't work without Zookeeper
  - Kafka 3.x can work without zookeeper (KIP-500) - using Kafka Raft instead 
  - Kafka 4.x will not have zookeeper
  - Zookeeper by desing operates with an odd number of servers (1,3,5,7)
  - zookeeper has a leader(writes) the rest of the servers are followers (reads)
  - zookeeper doesn't store consumer offsets with kafka > v0.10
  



    

